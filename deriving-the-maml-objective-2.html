<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <title>Deriving the MAML Objective - 2 - Vaasudev's Blog</title>
    <meta charset="utf-8" />
    <link href="https://fonts.googleapis.com/css?family=Arimo:400,700|Inika" rel="stylesheet" type="text/css" />
    <link rel="stylesheet" type="text/css" href="./theme/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="./theme/pastie.css" />
    <link href="https://vdevmcitylp.github.io/" type="application/atom+xml" rel="alternate" title="Vaasudev's Blog Atom Feed" />


    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [[ '$','$']],
          displayMath:  [['$$','$$']],
          processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script> 

    <script type="text/javascript"          
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>

  </head>

  <body>
    <div class="container">
      <div class="row">
        <div class="span8">
<div id="content">
    <div class="header">
        <h1>Deriving the MAML Objective - 2</h1>
    </div>
    <p class="meta"><small><span><a href="./author/vaasudev-narayanan/">Vaasudev Narayanan</a> - </span><span>Fri 08 November 2019</span> - <span class="tags"></span></small></p>
        <div class="entry-content">
        <p>In this post, we'll be deriving the meta-gradient of the <a href="https://arxiv.org/pdf/1703.03400.pdf">MAML</a> objective for the <em>Mean Squared Error (MSE)</em> loss. <br />
This is a follow-up to <a href="./deriving-the-maml-objective-1.html">Deriving the MAML Objective - 1.</a></p>
<p>In our setup, we have $n$ tasks and the meta-update is defined as,</p>
<p>$$
\begin{equation} \label{eq1}
    \theta := \theta - \beta \nabla_\theta\sum_{T_{i = 1}}^n L_{T_i}(f_{\theta_i'})
\end{equation}
$$</p>
<p>We are interested in calculating $ \nabla_\theta\sum_{T_{i = 1}}^n L_{T_i}(f_{\theta_i'}) $ where $L$ is the mean-squared error defined as,
$ \newcommand{\norm}[1]{\left\lVert #1 \right\rVert} $
$$
\begin{equation} \label{eq2}
    L_{T_i}(f_{\theta_i'}) = \frac{1}{2} \sum_{j = 1}^K \norm{f_{\theta_i'}(x^{(j)}) - y^{(j)}}_{2}^{2}
\end{equation}
$$</p>
<p>We will consider the simpler case of a single-output regression task. So, </p>
<p>$$
\begin{equation} \label{eq3}
    L_{T_i}(f_{\theta_i'}) = \frac{1}{2} \sum_{j = 1}^K (f_{\theta_i'}(x^{(j)}) - y^{(j)})^{2}
\end{equation}
$$</p>
<p>The gradient for the $ i^{th} $ task is defined as,</p>
<p>$$
\begin{equation} \label{eq4}
    \frac{\partial L_{T_i}(f_{\theta_i'})}{\partial \theta} = \frac{\partial L_{T_i}(f_{\theta_i'})}{\partial \theta_i'} \cdot
    \frac{\partial \theta_i'}{\partial \theta}
\end{equation}
$$</p>
<p>The first term of equation \ref{eq4}, $ \frac{\partial L_{T_i}(f_{\theta_i'})}{\partial \theta_i'} $ is the derivative of the task loss function w.r.t. to the task-adapted parameters,</p>
<p>$$ 
\begin{equation}
    \begin{split}
        \frac{\partial L_{T_i}(f_{\theta_i'})}{\partial \theta_i'} &amp;= \frac{\partial L_{T_i}(f_{\theta_i'})}{\partial f_{\theta_i'}} \cdot \frac{\partial f_{\theta_i'}}{\partial \theta_i'} \\
        &amp;= \sum_{k = 1}^K (f_{\theta_i'}(x^{(k)}) - y^{(k)}) \cdot \frac{\partial f_{\theta_i'}}{\partial \theta_i'} (x^{(k)})
    \end{split}
    \label{eq5}
\end{equation}
$$</p>
<p>K is the number of data-points for a task in a K-shot setting.
<br /></p>
<p>The second term of equation \ref{eq3}, $ \frac{\partial \theta_i'}{\partial \theta} $ is the derivative of the task-adapted parameters w.r.t. the meta-parameters.</p>
<p>To calculate $ \frac{\partial \theta_i'}{\partial \theta} $, let us assume that $ \theta_i' $ and $ \theta $ are both (D+1)-dimensional vectors,</p>
<p>$$
\begin{equation}
    \begin{split}
        &amp;\theta_i' = [\theta_{i0}', \theta_{i1}', \dots, \theta_{iD}'] \\
        &amp;\theta = [\theta_0, \theta_1, \dots, \theta_D]
    \end{split}
    \label{eq6}
\end{equation}
$$</p>
<p>Then the Jacobian is defined as,</p>
<p>$$
\begin{equation} \label{eq7}
    \frac{\partial \theta_i'}{\partial \theta} = 
    \begin{bmatrix}
        \frac{\partial \theta_{i0}'}{\partial \theta_0} &amp; \frac{\partial \theta_{i0}'}{\partial \theta_1} &amp; \dots &amp; \frac{\partial \theta_{i0}'}{\partial \theta_D} \\
        \frac{\partial \theta_{i1}'}{\partial \theta_0} &amp; \frac{\partial \theta_{i1}'}{\partial \theta_1} &amp; \dots &amp; \frac{\partial \theta_{i1}'}{\partial \theta_D} \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        \frac{\partial \theta_{iD}'}{\partial \theta_0} &amp; \frac{\partial \theta_{iD}'}{\partial \theta_1} &amp; \dots &amp; \frac{\partial \theta_{iD}'}{\partial \theta_D}
    \end{bmatrix}
\end{equation}
$$</p>
<p>By definition, </p>
<p>$$
\begin{equation} \label{eq8}
    \theta_{ij}' = \theta_j - \alpha \frac{\partial L_{T_i}(\theta)}{\partial \theta_j}
\end{equation}
$$</p>
<p>Calculating a few terms of the Jacobian,
$$
\begin{equation*}
    \frac{\partial \theta_{i0}'}{\partial \theta_0} = 1 - \alpha \frac{\partial^2 L_{T_i}(\theta)}{\partial^2 \theta_0} \\
    \frac{\partial \theta_{i0}'}{\partial \theta_1} = -\alpha \frac{\partial^2 L_{T_i}(\theta)}{\partial \theta_0 \partial \theta_1}
\end{equation*}
$$</p>
<p>Generalizing,</p>
<p>$$
\begin{equation} \label{eq9}
    \frac{\partial \theta_{ip}'}{\partial \theta_q} = 
    \begin{cases}
        1 - \alpha \frac{\partial^2 L_{T_i}(\theta)}{\partial^2 \theta_q}, &amp; \text{if } p = q \\
        -\alpha \frac{\partial^2 L_{T_i}(\theta)}{\partial \theta_p \partial \theta_q}, &amp; {p \neq q}
    \end{cases}
\end{equation}
$$
<!-- Double Derivative start -->
<br />
Let us calculate the double derivative term $ \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial^2 \theta} $,</p>
<p>$$
\begin{equation} \label{eq10}
    \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial^2 \theta} = 
    \begin{bmatrix}
        \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial^2 \theta_0} &amp; \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial \theta_0 \partial \theta_1} &amp; \dots &amp; \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial \theta_0 \partial \theta_D} \\
        \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial \theta_1 \partial \theta_0} &amp; \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial^2 \theta_1} &amp; \dots &amp; \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial \theta_1 \partial \theta_D} \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial \theta_D \partial \theta_0} &amp; \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial \theta_D \partial \theta_1} &amp; \dots &amp; \frac{\partial^2 L_{T_i}(f_{\theta})}{\partial^2 \theta_D}
    \end{bmatrix}
\end{equation}
$$</p>
<p>Thus, 
$$
\begin{equation} \label{eq11}
    \frac{\partial L_{T_i}(\theta)}{\partial \theta_q} = \sum_{j = 1}^K (f_\theta (x^{(j)}) - y^{(j)}) \cdot \frac{\partial f_{\theta}}{\partial \theta_q} (x^{(j)})
\end{equation}
$$</p>
<p>$$
\begin{equation} \label{eq12}
    \frac{\partial^2 L_{T_i}(\theta)}{\partial \theta_p \partial \theta_q} = \sum_{j = 1}^K [(f_\theta (x^{(j)}) - y^{(j)}) \cdot \frac{\partial^2 f_{\theta}}{\partial \theta_p \partial \theta_q} (x^{(j)}) + \frac{\partial f_{\theta}}{\partial \theta_p} (x^{(j)}) \cdot \frac{\partial f_{\theta}}{\partial \theta_q} (x^{(j)})]
\end{equation}
$$</p>
<!-- Double Derivative end -->

<p><br /></p>
<p>Therefore, using equation \ref{eq5}, \ref{eq8} &amp; \ref{eq12} we can now compute the gradient for the $i^{th}$ task w.r.t. the meta-parameters: $ \frac{\partial L_{T_i}(f_{\theta_i'})}{\partial \theta} $ (equation \ref{eq4})</p>
<p>$$
\begin{equation} \label{eq13}
    \frac{\partial L_{T_i}(f_{\theta_i'})}{\partial \theta} = \sum_{k = 1}^K (f_{\theta_i'}(x^{(k)}) - y^{(k)}) \cdot \frac{\partial f_{\theta_i'}}{\partial \theta_i'} (x^{(k)}) \cdot \frac{\partial \theta_i'}{\partial \theta}
\end{equation}
$$</p>
<p>Calculating the gradient for all the $n$ tasks &amp; summing them up will give us $ \nabla_\theta\sum_{T_{i = 1}}^n L_{T_i}(f_{\theta_i'}) $ (equation \ref{eq2})</p>
<p>We can now perform our meta-update,  </p>
<p>$$
\begin{equation*}
    \theta := \theta - \beta \nabla_\theta\sum_{T_{i = 1}}^n L_{T_i}(f_{\theta_i'})
\end{equation*}
$$</p>
<p><strong>References</strong>:</p>
<p>Chelsea Finn, Pieter Abbeel, and Sergey Levine. <a href="https://arxiv.org/pdf/1703.03400.pdf">“Model-agnostic meta-learning for fast adaptation of deep networks.”</a> ICML 2017.</p>
        </div><!-- /.entry-content -->
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'vdevmcitylp'; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
        </div>
        <div class="span3 offset1">
          <div class="well">
            <ul class="nav nav-list">
              <li class="nav-header">Blog</li>
              <li ><a href="./index.html">Home</a></li>
              <!-- <li ><a href="./tags/">Tags</a></li> -->
              <!-- <li ><a href="./archives/">Archiv</a></li> -->
              <li class="nav-header">Pages</li>
              <li><a href="./pages/about.html">About Me</a></li>
              <li><a href="./pages/publications.html">Publications</a></li>
              <li><a href="./pages/internships.html">Internships</a></li>
              <li class="nav-header">Links</li>
              <li><a href="https://drive.google.com/open?id=1SjP2Y6FXU9hyiwQ053Iv3BOwEmmIiU3k">CV</a></li>
              <li><a href="https://scholar.google.co.in/citations?user=ncXa5P0AAAAJ&hl=en">Google Scholar</a></li>
              <li><a href="https://github.com/vdevmcitylp">Github</a></li>
              <li><a href="https://www.linkedin.com/in/vaasudev-narayanan-659b6010b/">LinkedIn</a></li>
              <li><a href="https://twitter.com/VDev_AGI">Twitter</a></li>
            </ul>
          </div><!-- /#menu -->
        </div>
      </div>

      <hr />

     <!--  <div class="row">
        <div class="span12">
          <div id="about">
            <p>Proudly powered by <a href="http://twitter.github.com/bootstrap/">bootstrap</a>, <a href="http://docs.notmyidea.org/alexis/pelican/">pelican</a>, <a href="http://python.org">python</a> and <a href="http://www.julo.ch/about/">Alex</a>!</p>
          </div><!-- /#about -->
        <!--</div> /#contentinfo 
      </div>
    </div> -->

    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'vdevmcitylp'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'https://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>

  </body>
</html>